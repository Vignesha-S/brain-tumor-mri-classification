{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Brain Tumor MRI Image Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brain tumor diagnosis is a critical medical task that requires accurate and timely interpretation of MRI scans. Manual diagnosis can be time-consuming and prone to human error, especially when distinguishing between different types of tumors. This project aims to build a deep learning-based system to classify brain MRI images into four categories: **Glioma**, **Meningioma**, **Pituitary Tumor**, and **No Tumor**.\n",
        "\n",
        "We started by exploring and preprocessing the dataset, which included MRI scans labeled according to tumor type. Preprocessing steps included image resizing, normalization, and augmentation to enhance model performance and reduce overfitting. The dataset was split into training, validation, and test sets to ensure unbiased model evaluation.\n",
        "\n",
        "Two models were developed:\n",
        "\n",
        "1. **Custom CNN** – A convolutional neural network built from scratch using layers like Conv2D, MaxPooling, Dropout, and Dense. This model served as a baseline and helped us understand the effectiveness of a simple architecture.\n",
        "\n",
        "2. **MobileNetV2 (Transfer Learning)** – A lightweight pretrained model from ImageNet. We fine-tuned the top layers and added a custom classification head suitable for our 4-class problem. This model leveraged pretrained weights and significantly improved performance.\n",
        "\n",
        "Both models were trained using **EarlyStopping** and **ModelCheckpoint** to prevent overfitting and retain the best-performing versions. Performance was measured using metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "* The **Custom CNN** achieved 71% test accuracy, with relatively lower recall on meningioma cases.\n",
        "* **MobileNetV2** outperformed the custom model with **81% accuracy**, strong precision and recall across all tumor types, and better generalization.\n",
        "\n",
        "To make the model accessible, we deployed the MobileNetV2 model using **Streamlit**, a Python-based web framework. The app allows users to upload brain MRI images and view predictions along with confidence scores. The interface is simple, intuitive, and displays both the uploaded image and the predicted tumor type (e.g., Glioma, Pituitary).\n",
        "\n",
        "We tested the app manually with multiple MRI images, and it successfully predicted most cases with high confidence. This streamlines the diagnostic process and supports healthcare professionals with a tool that’s fast and reliable.\n",
        "\n",
        "In summary, this project showcases how deep learning, especially **transfer learning**, can be applied effectively to real-world medical imaging problems. With further improvements like larger datasets or tumor localization, this tool could become even more helpful in clinical settings."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Vignesha-S/brain-tumor-mri-classification"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brain tumors are among the most fatal forms of cancer. Early detection and accurate classification are crucial for timely treatment. However, manual analysis of MRI scans is time-consuming and prone to human error.\n",
        "This project focuses on building an automated classification system using deep learning that can assist healthcare professionals by identifying tumor types from MRI images with high accuracy and confidence."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Understand the Dataset***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive & Define Paths and Parameters"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset directories\n",
        "train_dir = '/content/drive/MyDrive/Labmentix_internship/Brain_Tumor_Image_Classification/dataset/train'\n",
        "val_dir = '/content/drive/MyDrive/Labmentix_internship/Brain_Tumor_Image_Classification/dataset/valid'\n",
        "test_dir = '/content/drive/MyDrive/Labmentix_internship/Brain_Tumor_Image_Classification/dataset/test'\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 4"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Data Visualization***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class Distribution Plot\n",
        "\n",
        "image = ImageDataGenerator(rescale=1./255).flow_from_directory(train_dir, target_size=IMG_SIZE)\n",
        "labels = list(image.class_indices.keys())\n",
        "\n",
        "# Count images per class\n",
        "train_counts = pd.Series(image.classes).value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(labels, train_counts)\n",
        "plt.title('Class Distribution in Training Set')\n",
        "plt.xlabel('Tumor Class')\n",
        "plt.ylabel('Image Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a bar chart to visualize the number of images in each class (glioma, pituitary, meningioma, no_tumor) to quickly assess class balance in the training dataset. Bar charts are ideal for comparing category-wise frequencies, making them a clear choice for understanding how the data is distributed across different tumor categories."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Glioma class has the highest number of training samples (approx. 550+).\n",
        "\n",
        "- The Pituitary and Meningioma classes follow with moderate counts.\n",
        "\n",
        "- The No Tumor class has the lowest number of images (approx. 300–350).\n",
        "This shows a moderate class imbalance, with the Glioma class being the most represented and No Tumor the least."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights are important for both model performance and business impact:\n",
        "\n",
        "*Positive Impact:*\n",
        "\n",
        "Understanding the class distribution allows us to proactively apply strategies like data augmentation or class weighting. This helps ensure that all tumor types, especially the underrepresented ones, are equally learned by the model. In the medical domain, balanced detection across all tumor types is essential for **accurate diagnosis and patient safety.**\n",
        "\n",
        "*Negative Growth if Ignored:*\n",
        "\n",
        "If the class imbalance is not addressed, the model may become biased toward the more common classes (e.g., Glioma), and fail to detect less frequent but critical conditions like No Tumor. This can lead to **false positives** or **missed diagnoses**, which are unacceptable in a healthcare scenario."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample MRI Images from Each Class\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "for label, idx in image.class_indices.items():\n",
        "  folder = os.path.join(train_dir, label)\n",
        "  img_file = random.choice(os.listdir(folder))\n",
        "  img_path = os.path.join(folder, img_file)\n",
        "  img = plt.imread(img_path)\n",
        "  plt.subplot(2, 2, idx + 1)\n",
        "  plt.imshow(img)\n",
        "  plt.title(label)\n",
        "  plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Sample MRI Images from Each Class\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used this image grid to display one sample MRI image from each tumor class. This chart helps us visually inspect the dataset and understand what kind of variations exist across different tumor types. In image classification tasks, it's important to get a feel for the visual **complexity, contrast**, and **features** that models might learn."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Each tumor class has **distinct visual features** (size, shape, location, and intensity patterns in the MRI).\n",
        "\n",
        "- The **No Tumor** class typically has more uniform brain structures, while **tumor classes** show clear irregularities.\n",
        "\n",
        "- Some classes like **Glioma** and **Meningioma** appear visually similar in some cases, which could be challenging for the model."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Positive Impact:*\n",
        "\n",
        "These insights help us understand what the model will be learning from.\n",
        "\n",
        "It tells us that **image resolution, contrast, and preprocessing** are important for success.\n",
        "\n",
        "It also guides the need for **data augmentation** to teach the model invariance to position, brightness, etc.\n",
        "\n",
        "*Possible Negative Growth if Ignored:*\n",
        "\n",
        "If the images are not **preprocessed consistently**, or if class-specific features are subtle and not augmented well, the model may struggle to generalize.\n",
        "\n",
        "Misclassifying visually similar tumors like Glioma and Meningioma could delay diagnosis or cause incorrect treatment, leading to **clinical risks**."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***3. Data Pre-processing & Augmentation***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Image Preprocessing (Normalization & Resizing)"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing for validation and test sets (rescaling)\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We used the *ImageDataGenerator* class to normalize all image pixel values from the range [0, 255] to [0, 1]. Additionally, all images are resized to a fixed shape of 224×224 to ensure uniformity and compatibility with the model input layer."
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Augmentation"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentaion + rescaling for training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        ")"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### To improve generalization and reduce overfitting, we applied real-time data augmentation techniques on the training dataset such as rotation, zoom, shift, and horizontal flip. These techniques simulate real-world variability in MRI scans."
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Create Image Generators"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created image generators for train, validation, and test sets using the above preprocessing pipelines. These generators efficiently load and preprocess images in batches during training and evaluation."
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Visualize Augmentation Results"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 5 augmemted images\n",
        "x_batch, y_batch = next(train_generator)\n",
        "plt.figure(figsize=(12, 3))\n",
        "for i in range(5):\n",
        "  plt.subplot(1, 5, i+1)\n",
        "  plt.imshow(x_batch[i])\n",
        "  plt.axis('off')\n",
        "plt.suptitle(\"Augmented Training Images\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To confirm the correctness of our augmentation pipeline, we visualized augmented samples from the training set. As shown below, each image preserves the original tumor pattern while introducing realistic variations in orientation and brightness."
      ],
      "metadata": {
        "id": "b1lNLnFCB5v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Model Building - Custom CNN***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build custom CNN\n",
        "custom_cnn = Sequential()\n",
        "\n",
        "# Layer 1\n",
        "custom_cnn.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "custom_cnn.add(BatchNormalization())\n",
        "custom_cnn.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "custom_cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "custom_cnn.add(BatchNormalization())\n",
        "custom_cnn.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "custom_cnn.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "custom_cnn.add(BatchNormalization())\n",
        "custom_cnn.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flatten + Dense\n",
        "custom_cnn.add(Flatten())\n",
        "custom_cnn.add(Dense(128, activation='relu'))\n",
        "custom_cnn.add(Dropout(0.5))\n",
        "custom_cnn.add(Dense(NUM_CLASSES, activation='softmax')) # 4 output classes"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile Model\n",
        "custom_cnn.compile(optimizer='adam',\n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "# Show summary\n",
        "custom_cnn.summary()"
      ],
      "metadata": {
        "id": "tV8Uhu1gFLZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We built a custom Convolutional Neural Network (CNN) using three convolutional blocks followed by dense layers. Batch normalization was applied after each convolutional layer to speed up convergence. Dropout was used before the final dense layer to prevent overfitting. The model outputs probabilities for 4 tumor classes using a softmax activation.\n"
      ],
      "metadata": {
        "id": "vaSjOunuFSNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Transfer Learning (MobilenetV2)***"
      ],
      "metadata": {
        "id": "02iukUqrIdH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Load & Customize MobileNetV2"
      ],
      "metadata": {
        "id": "mYh2RsSrImgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MobileNetV2 base model without the top layer\n",
        "base_model = MobileNetV2(input_shape=(224, 224, 3),\n",
        "                         include_top=False,\n",
        "                         weights='imagenet')\n",
        "\n",
        "# Freeze the base model (we'll fine-tune later)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom layers on top\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "# Final model\n",
        "mobilenet_model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "mobilenet_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                        loss='categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "# Summary\n",
        "mobilenet_model.summary()"
      ],
      "metadata": {
        "id": "-wOt0t7dI7kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning with MobileNetV2"
      ],
      "metadata": {
        "id": "JeQc2NunQxIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used **MobileNetV2**, a lightweight CNN pretrained on the ImageNet dataset, as a base for transfer learning. The top layers were removed and replaced with a global average pooling layer, a dense ReLU layer, a dropout layer, and a final softmax layer to classify into 4 tumor categories. Initially, we froze the base model to train only the top layers. Fine-tuning can be done later if needed."
      ],
      "metadata": {
        "id": "pZOUhJu6P-pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Model Training***"
      ],
      "metadata": {
        "id": "H-BUw4HERDFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Set Up Callbacks"
      ],
      "metadata": {
        "id": "4BkVeNMDRN0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model based on validation loss\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "checkpoint_custom = ModelCheckpoint('custom_cnn_best.h5', monitor='val_loss', save_best_only=True)\n",
        "checkpoint_mobilenet = ModelCheckpoint('mobilenetv2_best.h5', monitor='val_loss', save_best_only=True)"
      ],
      "metadata": {
        "id": "t5f-yqR6RVj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🔁 Checkpoints and Early Stopping\n",
        "\n",
        "To avoid overfitting and save the best performing model based on validation loss, we implemented:\n",
        "- **EarlyStopping**: Stops training if no improvement in validation loss after several epochs.\n",
        "- **ModelCheckpoint**: Saves the model with the best validation performance for future evaluation or deployment."
      ],
      "metadata": {
        "id": "GYCp6-cb0mWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Train Custom CNN Model"
      ],
      "metadata": {
        "id": "vLn4NGmhRzSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_custom = custom_cnn.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stop, checkpoint_custom]\n",
        ")"
      ],
      "metadata": {
        "id": "wiJeh1D0R5kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Train MobileNetV2 Model"
      ],
      "metadata": {
        "id": "LeQUCfFwSMyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_mobilenet = mobilenet_model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stop, checkpoint_mobilenet]\n",
        ")"
      ],
      "metadata": {
        "id": "ypqZA1biSRvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained models permanently to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/Labmentix_internship/Brain_Tumor_Image_Classification/\n",
        "!cp custom_cnn_best.h5 /content/drive/MyDrive/Labmentix_internship/Brain_Tumor_Image_Classification/\n",
        "!cp mobilenetv2_best.h5 /content/drive/MyDrive/Labmentix_internship/Brain_Tumor_Image_Classification/"
      ],
      "metadata": {
        "id": "t48MfPfL1BQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Plot Accuracy & Loss"
      ],
      "metadata": {
        "id": "CnwGYWK4Srkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history, model_name):\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs_range = range(len(acc))\n",
        "\n",
        "  plt.figure(figsize=(14, 5))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.title(f'{model_name} - Accuracy')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(epochs_range, loss, label='Training Loss')\n",
        "  plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.title(f'{model_name} - Loss')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Qixq46IDSqtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history_custom, \"Custom CNN\")\n",
        "plot_history(history_mobilenet, \"MobileNetV2\")"
      ],
      "metadata": {
        "id": "HE2kJ9JuUB_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6265nm4zzjsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We trained both our Custom CNN and the MobileNetV2 transfer learning model using EarlyStopping and ModelCheckpoint to monitor validation loss. Each model was trained for up to 20 epochs, with performance tracked across training and validation sets.\n",
        "\n",
        "Training/validation accuracy and loss curves are plotted below for each model to evaluate learning behavior and potential overfitting or underfitting.\n"
      ],
      "metadata": {
        "id": "zMZCsDE7UGt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ✅ Summary of Model Training\n",
        "\n",
        "- Both Custom CNN and MobileNetV2 were trained using the defined callbacks.\n",
        "- The best models were saved as `.h5` files: `custom_cnn_best.h5` and `mobilenetv2_best.h5`.\n",
        "- These saved models will be used later for **evaluation**, **comparison**, and **deployment**."
      ],
      "metadata": {
        "id": "p8HiGkms0HRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. Model Evaluation***"
      ],
      "metadata": {
        "id": "6wBRzmpBWIEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Load Best Model"
      ],
      "metadata": {
        "id": "FIhRij60WP1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load best saved models\n",
        "custom_cnn = load_model('custom_cnn_best.h5')\n",
        "mobilenet_model = load_model('mobilenetv2_best.h5')"
      ],
      "metadata": {
        "id": "CmRF-WxqWYnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Prepare Predictions for Evaluation"
      ],
      "metadata": {
        "id": "419mSlTKW2qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict labels\n",
        "y_pred_cnn = custom_cnn.predict(test_generator)\n",
        "y_pred_mobilenet = mobilenet_model.predict(test_generator)\n",
        "\n",
        "# Convert predictions from probabilities to class indices\n",
        "y_pred_cnn_labels = np.argmax(y_pred_cnn, axis=1)\n",
        "y_pred_mobilenet_labels = np.argmax(y_pred_mobilenet, axis=1)\n",
        "\n",
        "# True labels\n",
        "y_true = test_generator.classes"
      ],
      "metadata": {
        "id": "I5XaueR4W9k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Print Classification Report"
      ],
      "metadata": {
        "id": "MpB8D6ysXZns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class labels\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# For Custom CNN\n",
        "print(\"Custom CNN Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_cnn_labels, target_names=class_labels))\n",
        "\n",
        "# For MobileNetV2\n",
        "print(\"\\nMobileNetV2 Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_mobilenet_labels, target_names=class_labels))"
      ],
      "metadata": {
        "id": "3DCGYBvoXfiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Plot Confusion *Matrix*"
      ],
      "metadata": {
        "id": "Ou7xuD9KX4Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_conf_matrix(y_true, y_pred, model_name):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  plt.figure(figsize=(6,5))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels, cmap='Blues')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Actual')\n",
        "  plt.title(f'{model_name} - Confusion Matrix')\n",
        "  plt.show()\n",
        "\n",
        "plot_conf_matrix(y_true, y_pred_cnn_labels, \"Custom CNN\")\n",
        "plot_conf_matrix(y_true, y_pred_mobilenet_labels, \"MobileNetV2\")"
      ],
      "metadata": {
        "id": "zRPNYxkbX8_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluated both models using test set predictions. The classification report includes metrics such as accuracy, precision, recall, and F1-score for each of the four tumor classes: glioma, meningioma, pituitary, and no_tumor. Confusion matrices visualize the model's prediction performance for each category."
      ],
      "metadata": {
        "id": "2W5nNIdqaQCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model Used and Its Performance Using Evaluation Metric Score Chart"
      ],
      "metadata": {
        "id": "WyMdyRWDkCyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom CNN**\n",
        "- A model built from scratch using multiple *Conv2D, MaxPooling, Dropout,* and *Dense layers*.\n",
        "\n",
        "- **Evaluation on test data:**\n",
        "\n",
        "    - **Accuracy:** 71%\n",
        "\n",
        "    - **Macro F1-Score:** 66%\n",
        "\n",
        "    - **Precision:** Varies across classes; poor on meningioma.\n",
        "-**Observations:**\n",
        "\n",
        "    - Performs reasonably well on **glioma** and **no_tumor**.\n",
        "\n",
        "    - Shows very low recall for **meningioma**, indicating difficulty in distinguishing that class.\n",
        "\n",
        "    - Performance is unbalanced and affected by class overlap or limited representational power.\n",
        "\n",
        "\n",
        "**MobileNetV2 (Transfer Learning)**\n",
        "- Pretrained on ImageNet; fine-tuned with custom dense layers for brain tumor classification.\n",
        "\n",
        "- **Evaluation on test data:**\n",
        "\n",
        "    - **Accuracy:** 81%\n",
        "\n",
        "    - **Macro F1-Score:** 80%\n",
        "\n",
        "    - **Precision & Recall:** Strong performance across all 4 classes.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- Excellent recall on **pituitary** (1.00).\n",
        "\n",
        "- High precision and recall on **glioma** and **no_tumor**.\n",
        "\n",
        "- Balanced model suitable for real-world deployment."
      ],
      "metadata": {
        "id": "OrDGEApWkNmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "JgqYpRuGlyml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Used Techniques:**\n",
        "\n",
        "    - EarlyStopping: Stops training when validation loss doesn't improve.\n",
        "\n",
        "    - ModelCheckpoint: Saves the best-performing model based on validation accuracy.\n",
        "\n",
        "    - Manual selection of epochs and batch size.\n",
        "\n",
        "- **Why Not Advanced Tuning?**\n",
        "\n",
        "  - Time constraints and project focus were on architecture evaluation.\n",
        "\n",
        "  - For future work, techniques like **Keras Tuner, Optuna,** or **GridSearchCV** can be used for deeper optimization."
      ],
      "metadata": {
        "id": "mxFu4OH8l6zg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "EqSbufT9oLju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, MobileNetV2 significantly outperformed the custom CNN."
      ],
      "metadata": {
        "id": "wBTp3rTzoV_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8. Model Comparison***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compared the performance of two models: a Custom Convolutional Neural Network (CNN) and a Transfer Learning-based model using MobileNetV2.\n",
        "\n",
        "### Evaluation Summary\n",
        "\n",
        "| Metric        | Custom CNN | MobileNetV2 |\n",
        "|---------------|------------|-------------|\n",
        "| Accuracy      | 71%        | 81%         |\n",
        "| Precision     | 70%        | 82%         |\n",
        "| Recall        | 72%        | 81%         |\n",
        "| F1-Score      | 66%        | 80%         |\n",
        "\n",
        "### Confusion Matrix Comparison\n",
        "\n",
        "| Class       | CNN Correct Predictions | MobileNetV2 Correct Predictions |\n",
        "|-------------|--------------------------|---------------------------------|\n",
        "| Glioma      | 67 / 80                  | 72 / 80                         |\n",
        "| Meningioma  | 12 / 63                  | 36 / 63                         |\n",
        "| No Tumor    | 42 / 49                  | 37 / 49                         |\n",
        "| Pituitary   | 53 / 54                  | 54 / 54                         |\n",
        "\n",
        "### Finding:\n",
        "\n",
        "MobileNetV2 outperformed the custom CNN model across all evaluation metrics. It demonstrated better precision, recall, and F1-score, especially for challenging classes like Meningioma. Given its superior accuracy and generalization capability, **MobileNetV2** is identified as the most accurate, efficient, and reliable model for deployment.\n"
      ],
      "metadata": {
        "id": "nCL07uykqn55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***9. Streamlit Application Deployment***\n",
        "\n"
      ],
      "metadata": {
        "id": "oMlyrbP8xKdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We built an interactive web application using Streamlit that allows users to upload brain MRI images and get predictions about tumor type.\n",
        "\n",
        "### 💡 Features of the Web App\n",
        "\n",
        "- **Upload Interface**\n",
        "\n",
        "Users can upload MRI images (`.jpg`, `.jpeg`, `.png`) via a drag-and-drop file uploader.\n",
        "\n",
        "- **Image Display**\n",
        "\n",
        "The uploaded image is displayed on the screen for confirmation.\n",
        "\n",
        "- **Tumor Prediction**\n",
        "\n",
        "Once an image is uploaded, the model classifies the tumor type as one of:\n",
        "\n",
        "  - `Glioma`\n",
        "\n",
        "  - `Meningioma`\n",
        "\n",
        "  - `Pituitary`\n",
        "\n",
        "  - `No Tumor`\n",
        "\n",
        "- **Confidence Score**\n",
        "\n",
        "The app also displays the model’s confidence (in percentage) for the predicted class.\n",
        "\n",
        "- **Model Used**\n",
        "\n",
        "The predictions are made using the fine-tuned MobileNetV2 model loaded from a .h5 file.\n",
        "\n",
        "### ⚙️ Files Used in the App\n",
        "\n",
        "- `app.py` – Main Streamlit application.\n",
        "- `utils.py` – Helper functions for image preprocessing and prediction.\n",
        "- `mobilenetv2_best.h5` – Trained model used for prediction.\n",
        "- `requirements.txt` – Contains all the dependencies needed to run the app.\n",
        "\n",
        "### ▶️ To Run the App\n",
        "\n",
        "```bash\n",
        "\n",
        "streamlit run app.py"
      ],
      "metadata": {
        "id": "M4OeUKkyxFkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web App Interface – Screenshot**\n",
        "\n",
        "Below is the interface of our deployed **Brain Tumor MRI Classifier** built using Streamlit:\n",
        "\n",
        "![Streamlit UI](https://drive.google.com/uc?export=view&id=1IzG0kXP8xaLyH5PSsNLImRxGuuY-ty_p)"
      ],
      "metadata": {
        "id": "SoFbHuQ0M2EP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Manual Testing**\n",
        "\n",
        "To evaluate the real-world performance of our deployed brain tumor classification model, we manually tested it by uploading sample images from each tumor class through the Streamlit application.\n",
        "\n",
        "## **Manual Test Results:**"
      ],
      "metadata": {
        "id": "0MPcnHtdD7Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Test No. | Actual Label | Predicted Label | Confidence | Status      |\n",
        "| -------- | ------------ | --------------- | ---------- | ----------- |\n",
        "| 1        | MENINGIOMA   | MENINGIOMA      | 67.25%     | ✅ Correct   |\n",
        "| 2        | GLIOMA       | GLIOMA          | 96.35%     | ✅ Correct   |\n",
        "| 3        | NO\\_TUMOR    | NO\\_TUMOR       | 93.45%     | ✅ Correct   |\n",
        "| 4        | PITUITARY    | MENINGIOMA      | 52.14%     | ❌ Incorrect |\n",
        "| 5        | PITUITARY    | PITUITARY       | 94.15%     | ✅ Correct   |\n"
      ],
      "metadata": {
        "id": "Wz_RzUCSEAWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "- The model performed well across most classes, especially **GLIOMA**, **NO_TUMOR**, and **PITUITARY.**\n",
        "\n",
        "- Slight confusion was observed between **MENINGIOMA** and **PITUITARY**, possibly due to overlapping visual features.\n",
        "\n",
        "- Confidence scores provided additional insight into model certainty."
      ],
      "metadata": {
        "id": "T_-M2Yq_EfKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully developed a Brain Tumor MRI Classification system using deep learning techniques. A custom CNN and a pretrained MobileNetV2 model were implemented and evaluated. Based on accuracy and performance metrics, MobileNetV2 significantly outperformed the custom model.\n",
        "\n",
        "We then deployed our best model using a **Streamlit web application**, which allows users to upload MRI scans and get real-time tumor type predictions along with confidence scores. The app demonstrated strong performance across all tumor categories, with high accuracy and a user-friendly interface.\n",
        "\n",
        "### **Key Takeaways**\n",
        "\n",
        "* Custom CNN provided foundational understanding but showed limited generalization.\n",
        "* Transfer learning with MobileNetV2 improved both accuracy and model robustness.\n",
        "* The deployed Streamlit app enables accessible and practical tumor classification.\n",
        "* The entire pipeline from data preprocessing to deployment was completed successfully.\n",
        "\n",
        "This end-to-end solution can aid medical professionals by providing fast and reliable tumor classification support."
      ],
      "metadata": {
        "id": "E7xe21EmQueK"
      }
    }
  ]
}